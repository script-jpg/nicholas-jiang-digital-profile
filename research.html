<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title>Nicholas Jiang's Research</title>
<link rel="stylesheet" href="style.css">
<style>
  /* ----------  layout ---------- */
  .paper-card {
    display: flex;                   /* put image + text in one row  */
    gap: 1.75rem;                    /* space between the columns    */
    align-items: flex-start;         /* image and text align to top  */
    max-width: 1100px;               /* keep lines from getting huge */
    margin: 2rem auto;               /* center horizontally          */
    padding: 0 1rem;
  }

  /* ----------  left column (thumbnail) ---------- */
  .paper-card img {
    width: 180px;        /* tweak as you like */
    flex: 0 0 180px;     /* lock the column’s width */
    height: auto;
    border-radius: 6px;  /* soft corners look nicer */
  }

  /* ----------  right column (details) ---------- */
  .details { flex: 1; }                  /* take the remaining width */
  .title   { margin: 0 0 .5rem 0; }      /* tighten spacing */
  .abstract{ margin: 0 0 1rem 0; }

  /* authors / venue line  -------------------------------------- */
  .authors a {                           /* coloured author links   */
    text-decoration: none;
  }
  .authors a:hover { text-decoration: underline; }

  .venue   { font-style: italic; }

  /* action links “paper | code | bibtex” ------------------------ */
  .links a {
    text-decoration: none;
    margin-right: .5rem;
  }
  .links a:hover { text-decoration: underline; }

  /* ----------  responsiveness ---------- */
  @media (max-width: 600px) {
    .paper-card {
      flex-direction: column;            /* stack on small screens  */
      align-items: stretch;
    }
    .paper-card img {
      width: 140px;
      flex: none;                        /* let image shrink        */
      margin-bottom: 1rem;
    }
  }
</style>
</head>
<body>

  <h1 style="text-align: center;">Nicholas Jiang's Research Contributions</h1>
<div class="row" style="display: flex; flex-direction: row; justify-content: center; width: 100%; gap: 1rem;">
  <h2><a class="unstyled-link" href="index.html">Home</a></h2>
  <h2><a href="https://script-jpg.github.io/portfolio/" class="unstyled-link">Projects</a></h2>
  <h2><a class="unstyled-link" href="https://drive.google.com/file/d/1VZV7rR_ldmfQIk1-iUn8DZMM77Od6266/view">CV</a></h2>
</div>

 <hr style="margin: 1rem">

<article class="paper-card">
  <!-- left column -->
  <img src="img/wires.png" alt="=metaphor of non-overlapping wires" />

  <!-- right column -->
  <div class="details">
    <h2 class="title">The Unreasonable Effectiveness of Non‑Overlapping Failures in LLM Prover Ensembles</h2>

    <p class="abstract">
    Improving the reliability of mathematical reasoning in large language models (LLMs) is critical for applications in education, automated theorem proving, and formal verification. This paper investigates whether the functional diversity of prover models, specifically their non-overlapping failures, can be harnessed through ensembling to improve collective performance. We present a theoretical risk decomposition framework for an OR-aggregated ensemble of theorem provers, demonstrating that the ensemble's risk is equal to the average individual risk minus an 'ambiguity effect' that quantifies the diversity of the provers. Our analysis formalizes the intuition that diversity, defined as non-overlapping failures, is strictly beneficial in this context. We hypothesize that such ensembles may not only surpass the accuracy of any individual model but could also potentially generate proofs for statements previously unprovable by any single prover. Furthermore, we aim to investigate whether these techniques can be applied to current state-of-the-art models to push performance on more difficult, unsaturated benchmarks such as PutnamBench.
    </p>

    <p class="authors">
    <strong>Nicholas Jiang</strong>, Joe Zhou, Rishabh Sharma
    </p>

    <!-- <p class="venue">
      International Conference on Machine Learning, 2024.
    </p> -->

    <p class="links">
      <a href="https://drive.google.com/file/d/1UDnicvpUA_vR3d6C-CDziduUvPPcghw2/view?usp=sharing">Proposal</a>
      <!-- <a href="https://drive.google.com/file/d/1iI5C5-SiSii91iwVbWoxRqmUCRek_Zhq/view?usp=share_link">Video Presentation</a> -->
      <!-- <a href="#">code</a> |
      <a href="#">bibtex</a> -->
    </p>
  </div>
</article>

<article class="paper-card">
  <!-- left column -->
  <img src="img/diversity.png" alt="=visualization of different solvers" />

  <!-- right column -->
  <div class="details">
    <h2 class="title">Diversity-Driven Generalization in Mathematical Reasoning Ensembles</h2>

    <p class="abstract">
    We propose a framework for studying collaborative generalization in mathematical reasoning systems by training ensembles of solvers to learn from one another’s complete solutions in a fully self-supervised setting. Using the miniF2F benchmark, we construct ensembles with varying diversity, quantified via a novel Task2Vec-based Ensemble Diversity Coefficient (EDC). We fine-tune solvers on peer-generated proofs and evaluate generalization to held-out problems. We hypothesize that higher EDC predicts greater improvement, revealing diversity as a key factor in enabling ensemble-based peer learning.
    </p>


    <p class="authors">
    <strong>Nicholas Jiang</strong>, Joe Zhou, Rishabh Sharma, Sarvesh Sivakumar
    </p>

    <!-- <p class="venue">
      International Conference on Machine Learning, 2024.
    </p> -->

    <p class="links">
      <a href="https://drive.google.com/file/d/17V9SR4IQSnjB_zbUtWWcfBcQfp-g175Q/view?usp=sharing">Proposal</a>
      <a href="https://drive.google.com/file/d/1iI5C5-SiSii91iwVbWoxRqmUCRek_Zhq/view?usp=share_link">Video Presentation</a>
      <!-- <a href="#">code</a> |
      <a href="#">bibtex</a> -->
    </p>
  </div>
</article>
</body>
</html>
